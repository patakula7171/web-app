# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NEPxEGBviBbjltcBYeeEwYAfjwA2xwKt

## Linear Regresssion

How Linear Regression Works
Linear regression is one of the most basic and widely used machine learning algorithms. It is used to predict a continuous target variable (y) based on one or more input features (X). Here's a detailed explanation of how it works:

##### 1. The Mathematical Equation
Linear regression models the relationship between the dependent variable (y) and independent variables (X) as a linear equation:
    Linear regression models the relationship between the dependent variable (y) and independent variables (X) as a linear equation:

y=
β
0
+
β
1
x
1
+
β
2
x
2
+
⋯
+
β
n
x
n
y=β
0
 +β
1
 x
1
 +β
2
 x
2
 +⋯+β
n
 x
n

Where:

y
y: The predicted value (e.g., charges in the insurance dataset).

β
0
β
0
 : The intercept (value of
y
y when all
x
i
=
0
x
i
 =0).

β
1
,
β
2
,
…
,
β
n
β
1
 ,β
2
 ,…,β
n
 : Coefficients (weights) for each feature.

x
1
,
x
2
,
…
,
x
n
x
1
 ,x
2
 ,…,x
n
 : Independent variables (features like age, bmi, smoker, etc.).

The goal of linear regression is to find the values of
β
0
,
β
1
,
.
.
.
,
β
n
β
0
 ,β
1
 ,...,β
n
  that minimize the error between the predicted values (
y
pred
y
pred
 ) and the actual values (
y
true
y
true
 ).

Where:

y
true
,
i
y
true,i
 : The actual value for the
i
t
h
i
th
  data point.

y
pred
,
i
=
β
0
+
∑
j
=
1
n
(
β
j
x
i
j
)
y
pred,i
 =β
0
 +∑
j=1
n
 (β
j
 x
ij
 ): The predicted value for the
i
t
h
i
th
  data point.

OLS finds the line (or hyperplane in higher dimensions) that minimizes this error.

##### 2. How the Model Learns
Linear regression uses an optimization technique called Ordinary Least Squares (OLS) to minimize the sum of squared errors:


Error
=
∑
I
=
1
m
(
y
true
,
i
−
y
pred
,
i
)
2
Error=
i=1
∑
m
 (y
true,i
 −y
pred,i
 )
2

##### 3. Assumptions of Linear Regression
    For linear regression to perform well, certain assumptions must hold:

    Linearity: The relationship between features and the target variable is linear.

    Independence: Observations are independent of each other.

    Homoscedasticity: The variance of residuals (errors) is constant across all levels of the independent variables.

    Normality: Residuals are normally distributed.

    No Multicollinearity: Features should not be highly correlated with each other.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt


# Define features and target variable
X = data.drop('charges', axis=1)
y = data['charges']

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['age', 'bmi', 'children']),
        ('cat', OneHotEncoder(drop='first'), ['sex', 'smoker', 'region'])
    ]
)

# Apply preprocessing to features
X_processed = preprocessor.fit_transform(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

model = LinearRegression()
model.fit(X_train, y_train)

# Display model coefficients and intercept
print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)

# Make predictions on test set
y_pred = model.predict(X_test)

# Evaluate performance using R² and RMSE
r2 = r2_score(y_test, y_pred)
import numpy as np
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"R² Score: {r2:.3f}")
print(f"RMSE: ${rmse:,.2f}")

"""## Metrics
RMSE (Root Mean Square Error)
    Tells you the average difference between the values your model predicts and the actual values.

    It is in the same units as your target variable (for example, if predicting inssurance charges in dollars, RMSE is in dollars).

    Lower RMSE means your model's predictions are more accurate.

    RMSE is sensitive to outliers, as larger errors are squared and have a bigger impact on the result.

    Useful for comparing models on the same dataset or when you care about the size of prediction errors.

R-squared (R²)
    Tells you how much of the variability in the target variable your model can explain.

      Ranges from 0 to 1 (or 0% to 100%). An R² of 0 means the model explains none of the variability; an R² of 1 means it   explains all of it.

    R² is dimensionless, making it easier to compare across different datasets or models.

    A higher R² means your model fits the data better, but it does not tell you about the actual size of prediction errors.

    Less sensitive to outliers compared to RMSE

In Short:
RMSE tells you how much your predictions are off, on average.
R-squared tells you how much of the variation in the data your model can explain.
"""

# Plot actual vs predicted charges
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, color='skyblue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Reference line (y=x)
plt.xlabel('Actual Charges')
plt.ylabel('Predicted Charges')
plt.title('Actual vs Predicted Charges')
plt.show()

"""## Interpreting the Plot


    Each point: One patient from your test data.

    X-axis: The real insurance charge for that person (actual value).

    Y-axis: The insurance charge predicted by your model for that person (predicted value).

    Red dashed line: The "perfect prediction" line (where predicted = actual).

    If your model were perfect, every point would fall exactly on this line.


Points close to the red line:

    The model predicted the charge very accurately for those people.

Points above the red line:

    The model predicted a charge higher than the real charge (overprediction).

Points below the red line:

    The model predicted a charge lower than the real charge (underprediction).

Tight cluster around the line:

    Your model is generally accurate.

Points far from the line or scattered widely:

    The model is making big errors for those cases.

If you see a pattern (e.g., errors get bigger for higher charges), your model might not be fitting all ranges of the data equally .

### Exporting The Model
"""

import joblib

joblib.dump(model, 'insurance_model.pkl')
joblib.dump(preprocessor, 'insurance_preprocessor.pkl')

"""# Streamlit Library
Streamlit is an open-source Python framework that makes it easy for data scientists and machine learning engineers to build and share interactive web applications—especially for data analysis, visualization, and machine learning—using just a few lines of Python code.
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib

# Load the trained model and preprocessor
model = joblib.load('insurance_model.pkl')
preprocessor = joblib.load('insurance_preprocessor.pkl')

st.title("Insurance Charges Predictor")

st.write("Enter the details below to predict insurance charges:")

# User input widgets
age = st.number_input("Age", min_value=18, max_value=100, value=30)
sex = st.selectbox("Sex", options=['male', 'female'])
bmi = st.number_input("BMI", min_value=10.0, max_value=60.0, value=25.0, step=0.1)
children = st.number_input("Number of Children", min_value=0, max_value=5, value=0)
smoker = st.selectbox("Smoker", options=['yes', 'no'])
region = st.selectbox("Region", options=['northeast', 'northwest', 'southeast', 'southwest'])

if st.button("Predict Charges"):
    # Prepare input data as a DataFrame
    input_df = pd.DataFrame({
        'age': [age],
        'sex': [sex],
        'bmi': [bmi],
        'children': [children],
        'smoker': [smoker],
        'region': [region]
    })

    # Preprocess input
    input_processed = preprocessor.transform(input_df)

    # Predict
    prediction = model.predict(input_processed)[0]

    st.success(f"Predicted Insurance Charges: ${prediction:,.2f}")

import plotly.express as px
import pandas as pd

if st.checkbox("Show Interactive Regression Plot"):
    # Prepare data for plotly
    plot_df = pd.DataFrame({
        "Actual Charges": y_test,
        "Predicted Charges": y_pred
    })

    fig = px.scatter(
        plot_df,
        x="Actual Charges",
        y="Predicted Charges",
        title="Actual vs Predicted Charges (Interactive)",
        opacity=0.6,
        template="plotly_white"
    )

    # Add reference line
    fig.add_shape(
        type="line",
        x0=plot_df["Actual Charges"].min(),
        y0=plot_df["Actual Charges"].min(),
        x1=plot_df["Actual Charges"].max(),
        y1=plot_df["Actual Charges"].max(),
        line=dict(color="red", dash="dash")
    )

    st.plotly_chart(fig, use_container_width=True)


